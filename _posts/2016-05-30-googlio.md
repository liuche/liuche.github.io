---
title: googlio16
---

Google I/O was held at Shoreline Amphitheatre this year, and approximately 7,000 people attended. My biggest complaint was that several of the tracks were held in such tiny spaces that there was no reasonable way to get into many of the talks without skipping talks in the previous time slot in order to wait in line.

![waiting in lines]({{ site.baseurl }}/img/googlio/line-head.jpg "This is approximately 1/50th of a line.")

However, this meant I spent a lot of time chatting with people at the project booths, having conversations that ranged in topic from humanitarian crisis relief to education to amusing hack experiments. There are a lot of cool projects going on at the Googlphabet, and it was a lot of fun talking to people who were excited to answer my questions in minutiae. Fortunately, all the talks this year were [recorded](https://www.youtube.com/playlist?list=PLOU2XLYxmsILe6_eGvDN3GyiodoV3qNSC) (and I did manage to squeeze into a few good ones), so without further ado, here's a summary of my experience at I/O, loosely organized by topic.

## Android (of course)
Lots of people who go to I/O are Android developers or do Android-related work, so it's not surprising that a huge part of of this conference has a focus on Android. There were announcements of [new features in Android u(N)amed](https://www.youtube.com/watch?v=B08iLAtS3AQ), as well as deep-dives into existing Android frameworks.

### Image Compression
Colt McAnlis (of [#perfMatters](https://www.youtube.com/playlist?list=PLWz5rJ2EKKc9CBxr3BVjPTPoDPLdPIFCE)) gives another [talk](https://www.youtube.com/watch?v=r_LpCi6DQME) about performance, this time about tricks to save space for images. He starts off with a nice explanation of how PNG works, and then lists a bunch of tips:

* For simple images, consider lower-quality formats (e.g., 8-bit color for fewer colors)
* Use PNG optimizers (PNGQuant, ImageMagick, etc)
* aapt can sometimes bloat images, which you can turn off by setting <code>cruncherEnabled=false</code>
* Use vector drawables

There's also a cool-sounding image-comparison library called [Butteraugli](https://github.com/google/butteraugli) that helps find noticeable image degradation (by calculating a "psychovisual similarity" score).

### Themes and Styles
There was also a [useful talk](https://www.youtube.com/watch?v=TIHXGwRTMWI) about the themes and styles that Android uses. The attributes of styles and themes are basically key/value stores with type, and the slides had useful details about how android attribute lookup works, as well as some common pitfalls and best practices for their use.

### Fragments
Basically, Fragments should be used when you need their lifecycle management. In all other cases, you should probably just use a custom view.

Android N also adds <code>commitNow()</code> for committing transactions immediately to avoid [IllegalStateExceptions](http://www.androiddesignpatterns.com/2013/08/fragment-transaction-commit-state-loss.html) that happen when committing after <code>onSaveInstanceState()</code> has been called. The alternative has been to always call <code>executePendingTransactions()</code> (which could have side effects) or, in the worst case scenario, use <code>commitAllowingStateLoss()</code>. See the [talk](https://www.youtube.com/watch?v=k3IT-IJ0J98) for more details.

### Android Instant Apps
One new concept is using Android apps without needing to install them. In particular, this seems in direct competition to progressive web apps with Service Workers/Push notifications, but the use case seems pretty clear - every company and website and amusement park seems to be pushing an Android app these days, but oftentimes, you just need to this functionality once, or you don't really need the entire app.

Instant apps allow you to use "deep-links" (like Yelp links that will open the app to a specific restaurant) with all the nice functionality of the Android app, but without needing to already have the app installed. It's also possible to use a specific subset of the app's functionality, and there's no need to download the entire application.

To make an Instant App version of an app, developers need to split off a library of the specific functionality the IA provides. The IA still needs to request any permissions it needs. Some APIs (like making background services) are disabled, and the IA can't write to the SD card (to avoid loopholes like communicating with other IAs). I'm not really clear on how the Instant App gets cleaned up, but there's some general information on the [developer page]("https://developer.android.com/topic/instant-apps/index.html).

### Android Wear
I have to confess I stopped by the Android Wear table only because I'd missed a talk and the booth was Right There; I've always considered smart watches to be an annoying, unnecessary growth of phone addiction onto your wrist, where the only possible redeeming use case was providing navigation while bicycling, when it's inconvenient (and unsafe) to pull my phone out of a pocket to check if I've missed my turn yet.

However, there were some really cool things that came out of that conversation.

For one thing, Android Wear 2.0 will be able to function without needing to be constantly paired with a phone. My first response was "oh I guess it'll need its own data plan too" but as Arthur pointed out to me -- and this was mind-blowing to me -- <i>you don't need to have your phone with you at all times</i>.

I cannot count the number of times that I've pulled out my phone to read a text message or check my schedule, and then ended up 20 minutes later sucked into Facebook scrolling through pictures of people's food that I don't even care about or buying bizarre Japanese cat toys in Neko Atsume. Having tiny internet computers on our persons has changed the way people contextualize and understand the world around them, but when did this tool for communication and supplementing knowledge become the symbol for ultimate distraction?

In the Android Wear worldview, the watch will strike the balance of supplementing your daily life with information and communication without drowning you in distraction. Maps, text/calling, and Lyft - what more do you really need? Maybe a nice camera. I do understand that I'm maybe not the typical user in my resentment of distraction, and I could just uninstall all those other apps from my phone, but I love the idea of technology being a tool for humans, rather than a taskmaster or an addiction.

As an aside, there's also a new tiny Swypeable keyboard. Despite being rather comically small, it worked remarkably well, and its tiny size actually made it easier to slide my finger across than some of the behemoth phones that people carry around nowadays. For someone who finds talking to your phone rude and entirely too public for comfort (and whisper detection doesn't work too well thus far, I've tried), this means I could actually use a smartwatch without dying of embarrassment.

## Web
### Rich Cards in Search + schema.org
Rich Cards are special snippets from Google search results that present the content in a richer format, emphasizing key details and/or images. These cards show relevant information (such as ratings, calories, prep time), which is parsed from different search results.

![cake search]({{site.baseurl}}/img/googlio/chocolate-cake.png =350x "1 hour and 157 calories later...")

This is possible because websites can present schemas that semantically describe their content, by using [schema.org](http://schema.org/Recipe). This is pretty cool, and it looks like something that [DuckDuckGo](https://duckduckgo.com/?q=cake&ia=recipes) also incorporates.

For providers of content, there's also a tool for checking schema validity of your page/code snippet called [Structured Data Testing Tool](https://developers.google.com/search/docs/guides/prototype#test_markup_in_the_editor).

### Accelerated Mobile Pages (AMP) ⚡️
AMP was one of the most exciting things I saw at Google I/O. It's a [(small) library + HTML markup](https://www.ampproject.org/docs/get_started/create/basic_markup.html) that optimizes pages for loading quickly. It's browser agnostic, and intended mainly for static content (although pages can certainly provide updated content).

For normal web pages, it's hard for browsers to determine what content should be rendered first, or what can be delayed. AMP kind of acts like an intermediary, displaying structured content with a goal of optimizing for speed. It requires sizes for the elements of a page and builds the structure, and then loads or prerenders content as specified by the markup. Elements can't resize themselves unless the user interacts with them (so content won't jump around), and pages can still be responsive (but AMP will handle that).

As an example, the Washington Post has made a [progressive web app](https://www.washingtonpost.com/pwa) that uses AMP, and it really breaks the expectation that news sites never load quickly.

[This talk](https://www.youtube.com/watch?v=cfekj564rs0&list=PLOU2XLYxmsILe6_eGvDN3GyiodoV3qNSC) that gives an overview of how AMP works, and there's also a handy [text-version summary](https://www.ampproject.org/docs/get_started/technical_overview.html).

## Education
Blockly is a visual programming library that uses draggable blocks to represent code syntax and can generate code in some common languages (JS, Python, etc), as well as be augmented to generate code in other languages.

Visual programming is generally used by non-technical people to accomplish coding tasks, because the nature of the blocks and the way they fit together will not allow compile errors. I'm not sure how commonly it's actually used, but this does make it a good tool for teaching children basic coding.

At the booth, you could build Blockly programs on a tablet and then execute them on cute little robots, controlling their movement, rotation, or waving their arms. This was probably a pretty cool introduction to programming that omits a lot of the intimidating work of setting up a toolchain, compiling, fixing compile errors, etc.

![robots]({{ site.baseurl }}/img/googlio/robot.jpg =300x "A Blockly-programmed robot")

There's also a [Blockly Games webapp](https://blockly-games.appspot.com/), which is a lesson series that starts by introducing coding with Blockly, then moves to simple coding concepts like if/for loops, and then rapidly progresses to more challenging topics (such as path-finding, genetic algorithms, etc).

The group is also trying to make a version of Blockly that works for visually impaired people - the engineer mentioned that they'd talked to many blind/visually-impaired engineers at Google, and instead of treating code as blocks of text, they tended to think of it as a tree.

The coolest thing I saw at this booth was this simple little robot called an Ozobot. It only had a photosensor - no bluetooth, wifi. However, when you [write a program](h$$ttp://ozoblockly.com/editor) with Blockly, the page installs the program by flashing the screen to send the program to the device. It was a clever and also very visual simplification of how code gets "sent" between devices.

![Ozobot]({{ site.baseurl }}/img/googlio/ozobot.jpg =500x "Installing a Blockly program onto Ozobot")

## VR
VR is cool. VR games look awesome. There was an [entertaining talk](https://www.youtube.com/watch?v=lGUmTQgbiAY&list=PLOU2XLYxmsILe6_eGvDN3GyiodoV3qNSC) where some guys from DayDream Labs (which does rapid VR prototyping) discuss some interesting observations they made while running VR experiments.

* VR controllers are very intuitive because humans are used to using their hands to manipulate tools. Relatedly, people love throwing things - breaking rules is (maybe unsurprisingly) very fun.
* Head + hands avatars work better than full-body avatars. The head and hands are natural areas to render because VR usually tracks them already; rendering a full-body avatar is much more difficult, because as people move around, guessing how the rest of their bodies follow ends up being jarring and unsettling.
* Social interactions in VR are more visceral - which can be good and bad. People enjoy the sense of human connection (e.g., they loved giving high-fives with haptic feedback), but because it's so immersive, social VR should be designed with limiting bad behavior in mind, and ensure people feel comfortable in their personal space.
![avatar]({{site.baseurl}}/img/googlio/avatar.png "From the Daydream Labs talk")

I'm intrigued about whether the intuitiveness of VR controllers are extremely intuitive will make gaming more accessible to a wider variety of demographics due to its lower learning curve.

(They also suggest a talk on the [VR Design Process](https://www.youtube.com/watch?v=-mcXAMDch7s&list=PLOU2XLYxmsILe6_eGvDN3GyiodoV3qNSC&index=76) which I didn't see, but also sounds interesting.)

## Lightning talks
Ignite was a series of very quick and fascinating talks that loosely fall under the umbrella of "interesting impacts technology has on humans". A few of these topics:

* Workers' relation to code (e.g., programmers as manangers of code vs code as managers of Uber drivers)
* Using limitations to solve problems, like gamifying early prototypes to accomplish their goals (Waze turning streets into a PacMan game)
* Filter bubbles and the losing battle of facts over sensationalism

I couldn't find the video of this session, but it should eventually show up in the [schedule](https://events.google.com/io2016/schedule?sid=b8b6e442-2310-e611-a517-00155d5066d7#day2/b8b6e442-2310-e611-a517-00155d5066d7) if it gets put up.

## Connected Devices
Nest is now part of Alphabet, and they were showcasing The Connected Home - your speakers and entertainment system, lightbulbs, home surveillance cameras, air freshener (and of course your Nest thermostat and smoke detector). They also have a partnership with Yale (the lock manufacturer, not the university) to build an Internet-of-Things lock that can be installed in place of a normal lock and be connected to your presence/phone, although it doesn't quite work yet.

However, the vision of bringing home a new device, and having it nest-le in nicely with your smartfridge and your brave little toaster is not quite here -- right now, you need to install sixteen apps to control your sixteen devices, and they almost certainly don't talk to each other.

To address this problem, Nest is promoting a low-power-long-range mesh network protocol for IoT mesh communication, called Thread.

The common forms of wireless communication won't work for connected devices: connecting to WiFi is too power-intensive for battery-powered devices and requires proximity to a router, and Bluetooth is too short-range and can't support a large number of devices. Low-power long-range mesh networks are necessary.

Mesh network specifications like this do exist, such as Z-Wave (which is not open), and [Zigbee](https://en.wikipedia.org/wiki/ZigBee) (which is an open protocol, but not pure mesh and not IPv6).

Nest has built a consortium of 200+ companies called Thread Group ([partnered with Zigbee](http://www.zigbee.org/zigbee-alliance-press-release-zigbee-alliance-and-thread-group-collaborate-to-aid-development-of-connected-home-products/)), and has signed on some chip manufacturers to write drivers that implement the Thread protocol. They also released an open source implementation of Thread, called [OpenThread](https://github.com/openthread/openthread/blob/master/README.md).

Ironically, the Thread protocol hasn't been released yet (and is currently only available through paid membership) but the Nest team claims it would be released as an open standard soon. So there's always the challenge of promoting open standards.

## google.org

google.org is Google's philanthropic organization and they also had a booth, showcasing some work they were doing with Mercy Corp/International Rescue Committee for refugees, some accessibility hackathon projects, and other collaborations. They currently have about $100 million a year in funding to distribute.

Their criteria for humanitarian efforts are:

* Original application of technology
* Potential to impact at scale

They do this through a combination of providing funding and developers; they've also run hackathons (such as for accessibility) to try to find novel ideas to develop.

I was excited to see that one of their projects was helping provide an open standard for vision-imparied interfaces and guidelines in cities and public infrastructure - standards aren't as sexy as demos, but are far-reaching and impactful.
